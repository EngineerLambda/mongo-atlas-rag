{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Populate MongoDB Atlas Database\n",
    "In this Python notebook, we will be generating embeddings for two 10K filing documents, indexing them and storing them into MongoDB Atlas database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Setup\n",
    "We'll start with some basic setup steps in the next two code cells. Don't worry if your device does not have a GPU, you will still be able to proceed with the Quest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using CUDA/GPU:  False\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is enabled\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# To disable GPU and experiment, uncomment the following line\n",
    "# Normally, you would want to use GPU, if one is available\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "\n",
    "print (\"using CUDA/GPU: \", torch.cuda.is_available())\n",
    "\n",
    "for i in range(torch.cuda.device_count()):\n",
    "   print(\"device \", i , torch.cuda.get_device_properties(i).name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging. To see more logging, set the level to DEBUG\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Inspecting the Documents\n",
    "\n",
    "As mentioned earlier in the quest, we'll be using two 10K filings as our corpus. These are financial documents filed by US public companies to SEC (Securities and Exchange Commission). You can read about them [here](https://www.investor.gov/introduction-investing/investing-basics/glossary/form-10-k).\n",
    "\n",
    "The two 10K filings we'll be using are Lyft and Uber. You can find these documents in the `data` folder:\n",
    "```text\n",
    "data/10k\n",
    "├── lyft_2021.pdf\n",
    "└── uber_2021.pdf\n",
    "```\n",
    "\n",
    "Don't be fooled by the fact that it is just 2 documents - each PDF document is over 200+ pages long! These aren't any ordinary simple text documents, these are very dense text documents containing a lot of very specific information. \n",
    "\n",
    "Go ahead and inspect these documents manually before proceeding with this notebook to get a feel for the kinds of documents we're dealing with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATLAS_URI Connection string found\n"
     ]
    }
   ],
   "source": [
    "# Load settings from .env file\n",
    "from dotenv import find_dotenv, dotenv_values\n",
    "\n",
    "# Change system path to root direcotry\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "# _ = load_dotenv(find_dotenv()) # read local .env file\n",
    "config = dotenv_values(find_dotenv())\n",
    "\n",
    "# For debugging purposes\n",
    "# print (config)\n",
    "\n",
    "ATLAS_URI = config.get('ATLAS_URI')\n",
    "\n",
    "if not ATLAS_URI:\n",
    "    raise Exception (\"'ATLAS_URI' is not set.  Please set it above to continue...\")\n",
    "else:\n",
    "    print(\"ATLAS_URI Connection string found\")\n",
    "\n",
    "## Only uncomment this if you are using OpenAI for embeddings\n",
    "# OPENAI_API_KEY = config.get(\"OPENAI_API_KEY\")\n",
    "# if not OPENAI_API_KEY:\n",
    "#     raise Exception (\"'OPENAI_API_KEY' is not set. Please set it above to continue...\")\n",
    "# else:\n",
    "#     print(\"ATLAS_URI Connection string found:\", ATLAS_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our variables\n",
    "DB_NAME = 'abdulabiola21'\n",
    "COLLECTION_NAME = 'ele-562'\n",
    "INDEX_NAME = 'lambda_embedding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LlamaIndex will download embeddings models as needed\n",
    "# Set llamaindex cache dir to ../cache dir here (Default is system tmp)\n",
    "# This way, we can easily see downloaded artifacts\n",
    "os.environ['LLAMA_INDEX_CACHE_DIR'] = os.path.join(os.path.abspath('../'), 'cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atlas client initialized\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "\n",
    "mongodb_client = pymongo.MongoClient(ATLAS_URI)\n",
    "\n",
    "print (\"Atlas client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Clear out the collection\n",
    "\n",
    "In the event you're repeating this Quest again, run the code cell below for a fresh start! This code cell will delete the documents you've previously added to the `rag1 - 10k` collection (if any). If you are running this Quest for the first time, you should expect to see `0` below because you've yet to add any items into this collection yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:pymongo.serverSelection:{\"message\": \"Waiting for suitable server to become available\", \"selector\": \"Primary()\", \"operation\": \"count\", \"topologyDescription\": \"<TopologyDescription id: 6639485a892cc1eeb9780eeb, topology_type: ReplicaSetNoPrimary, servers: [<ServerDescription ('ac-rtbkmtd-shard-00-00.mmc8nho.mongodb.net', 27017) server_type: Unknown, rtt: None>, <ServerDescription ('ac-rtbkmtd-shard-00-01.mmc8nho.mongodb.net', 27017) server_type: Unknown, rtt: None>, <ServerDescription ('ac-rtbkmtd-shard-00-02.mmc8nho.mongodb.net', 27017) server_type: Unknown, rtt: None>]>\", \"clientId\": {\"$oid\": \"6639485a892cc1eeb9780eeb\"}, \"remainingTimeMS\": 29}\n",
      "{\"message\": \"Waiting for suitable server to become available\", \"selector\": \"Primary()\", \"operation\": \"count\", \"topologyDescription\": \"<TopologyDescription id: 6639485a892cc1eeb9780eeb, topology_type: ReplicaSetNoPrimary, servers: [<ServerDescription ('ac-rtbkmtd-shard-00-00.mmc8nho.mongodb.net', 27017) server_type: Unknown, rtt: None>, <ServerDescription ('ac-rtbkmtd-shard-00-01.mmc8nho.mongodb.net', 27017) server_type: Unknown, rtt: None>, <ServerDescription ('ac-rtbkmtd-shard-00-02.mmc8nho.mongodb.net', 27017) server_type: Unknown, rtt: None>]>\", \"clientId\": {\"$oid\": \"6639485a892cc1eeb9780eeb\"}, \"remainingTimeMS\": 29}\n",
      "Document count before delete : 0\n",
      "Deleted docs : 0\n"
     ]
    }
   ],
   "source": [
    "database = mongodb_client[DB_NAME]\n",
    "collection = database [COLLECTION_NAME]\n",
    "\n",
    "doc_count = collection.count_documents (filter = {})\n",
    "print (f\"Document count before delete : {doc_count:,}\")\n",
    "\n",
    "result = collection.delete_many(filter= {})\n",
    "print (f\"Deleted docs : {result.deleted_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Setup Embeddings\n",
    "\n",
    "Now, we'll need to establish an embedding model to help us generate embeddings for the documents we'll be uploading. Here, we provide two options - the first option is to use an OpenAI embedding model and the second option is to use an open source HuggingFace embedding model. We'll be going with the second approach here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - Option B : Using Custom Embeddings\n",
    "\n",
    "This option utilizes a HuggingFace embedding model. Recap from Quests 3 and 4 that you can choose from a large number of options. Listed below are some examples, taken from the leaderboard https://huggingface.co/spaces/mteb/leaderboard. We'll be going with the `BAAI/bge-small-en-v.15` embedding model here.\n",
    "\n",
    "| model name                              | overall score | model size | model params | embedding length | License  | url                                                            |\n",
    "|-----------------------------------------|---------------|------------|--------------|------------------|----------|----------------------------------------------------------------|\n",
    "| BAAI/bge-large-en-v1.5                  | 64.x          | 1.34 GB    | 335 M        | 1024             | MIT      | https://huggingface.co/BAAI/bge-large-en-v1.5                  |\n",
    "| BAAI/bge-small-en-v1.5                  | 62.x          | 133 MB     | 33.5 M       | 384              | MIT      | https://huggingface.co/BAAI/bge-small-en-v1.5                  |\n",
    "| sentence-transformers/all-mpnet-base-v2 | 57.8          | 438 MB     |              | 768              | Apache 2 | https://huggingface.co/sentence-transformers/all-mpnet-base-v2 |\n",
    "| sentence-transformers/all-MiniLM-L12-v2 | 56.x          | 134 MB     |              | 384              | Apache 2 | https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2 |\n",
    "| sentence-transformers/all-MiniLM-L6-v2  | 56.x          | 91 MB      |              | 384              | Apache 2 | https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:NumExpr defaulting to 4 threads.\n",
      "NumExpr defaulting to 4 threads.\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n",
      "Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n",
      "INFO:sentence_transformers.SentenceTransformer:2 prompts are loaded, with the keys: ['query', 'text']\n",
      "2 prompts are loaded, with the keys: ['query', 'text']\n"
     ]
    }
   ],
   "source": [
    "# from llama_index.embeddings import HuggingFaceEmbedding\n",
    "# Uncomment the line above and comment the line below if you face an import error\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_74881/3880219279.py:11: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "## Set up embedding model\n",
    "# The LLM used to generate natural language responses to queries\n",
    "# If not provided, it will default to gpt-3.5-turbo from OpenAI\n",
    "# If your OpenAI API key is not set, it will default to llama2-chat-13B from Llama.cpp\n",
    "# Since we don't need an LLM just yet, we'll be setting it to None\n",
    "\n",
    "# from llama_index import ServiceContext\n",
    "# Uncomment the line above and comment away the line below if you face an import error\n",
    "from llama_index.core import ServiceContext\n",
    "\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Connect Llama-Index and MongoDB Atlas\n",
    "\n",
    "As mentioned in the Quest, we'll be using MongoDB Atlas as our vector storage. This is critical to store indexed data and then query later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.mongodb import MongoDBAtlasVectorSearch\n",
    "# from llama_index.storage.storage_context import StorageContext\n",
    "# Uncomment the line above and comment away the line below if you face an import error\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "vector_store = MongoDBAtlasVectorSearch(mongodb_client = mongodb_client,\n",
    "                                 db_name = DB_NAME, collection_name = COLLECTION_NAME,\n",
    "                                 index_name  = INDEX_NAME,\n",
    "                                 ## The following columns are set to default values\n",
    "                                 # embedding_key = 'embedding', text_key = 'text', metadata_= 'metadata',\n",
    "                                 )\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Read PDF Documents\n",
    "\n",
    "Llama-index has very handy `SimpleDirectoryReader` that can read single/multiple files and also an entire directory's content. We'll be using this to read our 2 PDF files and storing the data in `docs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 128 chunks from '../data/ELE562'\n",
      "CPU times: user 11.8 s, sys: 87.7 ms, total: 11.8 s\n",
      "Wall time: 11.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# from llama_index.readers.file.base import SimpleDirectoryReader\n",
    "# Uncomment the line above and comment away the line below if you face an import error\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "data_dir = '../data/ELE562'\n",
    "\n",
    "docs = SimpleDirectoryReader(\n",
    "        input_dir=data_dir\n",
    ").load_data()\n",
    "\n",
    "print (f\"Loaded {len(docs)} chunks from '{data_dir}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Index the Documents and Store Them Into MongoDB Atlas\n",
    "\n",
    "The code cell below is where everything that we've been preparing for in this Python notebook comes together:\n",
    "- Embeddings are generated using our packaged-up embedding model `service_context` \n",
    "- Our documents `docs` get indexed `storage_context` - both text and embeddings are stored in MongoDB Atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43cd243f42f94a4f95c499e8e6b9d626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3d4cbb9133e45419c16204a63555560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7971dc009ba04560b1d70f1246674734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d653d14bbb44398bebb88066016ec7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "054a83688893484ebca6493a8aaab915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7177cddae09a4278a3dbee47a41ffe40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f48edd8202461684a682a202f7d922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5de504a933394eae8b58d1aa2b92d8c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e3db421ce4e42ebb235c74ad2dc2a38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "005f29dd1ad8415090a342e2e6740082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae858d05f5d545068c81cee025b57cc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9486accedf814b19ae6fe19d9d674edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d11e85969643e49bba4bfd0e48a4a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd9d7e63bda4372a5923beb551b7905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 41s, sys: 45.9 s, total: 2min 27s\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# from llama_index.indices.vector_store.base import VectorStoreIndex\n",
    "# Uncomment the line above and comment away the line below if you face an import error\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    docs, \n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Atlas env",
   "language": "python",
   "name": "atlas-1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

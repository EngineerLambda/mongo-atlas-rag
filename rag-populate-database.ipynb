{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Populate MongoDB Atlas Database\n",
    "In this Python notebook, we will be generating embeddings for a course material/document, indexing it and storing it into MongoDB Atlas database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATLAS_URI Connection string found\n"
     ]
    }
   ],
   "source": [
    "# Load settings from .env file\n",
    "import sys\n",
    "from dotenv import find_dotenv, dotenv_values\n",
    "\n",
    "# Change system path to root direcotry\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "# _ = load_dotenv(find_dotenv()) # read local .env file\n",
    "config = dotenv_values(find_dotenv())\n",
    "\n",
    "\n",
    "ATLAS_URI = config.get('ATLAS_URI')\n",
    "\n",
    "if not ATLAS_URI:\n",
    "    raise Exception (\"'ATLAS_URI' is not set.  Please set it above to continue...\")\n",
    "else:\n",
    "    print(\"ATLAS_URI Connection string found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our variables\n",
    "DB_NAME = 'abdulabiola21'\n",
    "COLLECTION_NAME = 'ele-562'\n",
    "INDEX_NAME = 'lambda_embedding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LlamaIndex will download embeddings models as needed\n",
    "# Set llamaindex cache dir to ../cache dir here (Default is system tmp)\n",
    "# This way, we can easily see downloaded artifacts\n",
    "os.environ['LLAMA_INDEX_CACHE_DIR'] = os.path.join(os.path.abspath('../'), 'cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atlas client initialized\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "\n",
    "mongodb_client = pymongo.MongoClient(ATLAS_URI)\n",
    "\n",
    "print (\"Atlas client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Clear out the collection\n",
    "\n",
    "In the event you're repeating this Quest again, run the code cell below for a fresh start! This code cell will delete the documents you've previously added to the `abdulabiola21-ele-562` collection (if any). If you are running this Quest for the first time, you should expect to see `0` below because you've yet to add any items into this collection yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:pymongo.serverSelection:{\"message\": \"Waiting for suitable server to become available\", \"selector\": \"Primary()\", \"operation\": \"count\", \"topologyDescription\": \"<TopologyDescription id: 663e2e750ba9f33da19ff21f, topology_type: ReplicaSetNoPrimary, servers: [<ServerDescription ('ac-rtbkmtd-shard-00-00.mmc8nho.mongodb.net', 27017) server_type: Unknown, rtt: None>, <ServerDescription ('ac-rtbkmtd-shard-00-01.mmc8nho.mongodb.net', 27017) server_type: Unknown, rtt: None>, <ServerDescription ('ac-rtbkmtd-shard-00-02.mmc8nho.mongodb.net', 27017) server_type: Unknown, rtt: None>]>\", \"clientId\": {\"$oid\": \"663e2e750ba9f33da19ff21f\"}, \"remainingTimeMS\": 29}\n",
      "{\"message\": \"Waiting for suitable server to become available\", \"selector\": \"Primary()\", \"operation\": \"count\", \"topologyDescription\": \"<TopologyDescription id: 663e2e750ba9f33da19ff21f, topology_type: ReplicaSetNoPrimary, servers: [<ServerDescription ('ac-rtbkmtd-shard-00-00.mmc8nho.mongodb.net', 27017) server_type: Unknown, rtt: None>, <ServerDescription ('ac-rtbkmtd-shard-00-01.mmc8nho.mongodb.net', 27017) server_type: Unknown, rtt: None>, <ServerDescription ('ac-rtbkmtd-shard-00-02.mmc8nho.mongodb.net', 27017) server_type: Unknown, rtt: None>]>\", \"clientId\": {\"$oid\": \"663e2e750ba9f33da19ff21f\"}, \"remainingTimeMS\": 29}\n",
      "Document count before delete : 0\n",
      "Deleted docs : 0\n"
     ]
    }
   ],
   "source": [
    "database = mongodb_client[DB_NAME]\n",
    "collection = database [COLLECTION_NAME]\n",
    "\n",
    "doc_count = collection.count_documents (filter = {})\n",
    "print (f\"Document count before delete : {doc_count:,}\")\n",
    "\n",
    "result = collection.delete_many(filter= {})\n",
    "print (f\"Deleted docs : {result.deleted_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Setup Embeddings\n",
    "\n",
    "Using Google's free embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index-embeddings-gemini -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"] = config.get(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:NumExpr defaulting to 4 threads.\n",
      "NumExpr defaulting to 4 threads.\n",
      "The dimension of the embedding model is: 768\n"
     ]
    }
   ],
   "source": [
    "# from llama_index.embeddings import HuggingFaceEmbedding\n",
    "# Uncomment the line above and comment the line below if you face an import error\n",
    "# imports\n",
    "from llama_index.embeddings.gemini import GeminiEmbedding\n",
    "\n",
    "embed_model = GeminiEmbedding(model_name=\"models/embedding-001\")\n",
    "\n",
    "# Test the embedding model to see the dimension in order to create the index with the right dimension on the vector db\n",
    "vector = embed_model.get_text_embedding(\"What is the meaning of life\")\n",
    "print(f\"The dimension of the embedding model is: {len(vector)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23827/3880219279.py:11: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "## Set up embedding model\n",
    "# The LLM used to generate natural language responses to queries\n",
    "# If not provided, it will default to gpt-3.5-turbo from OpenAI\n",
    "# If your OpenAI API key is not set, it will default to llama2-chat-13B from Llama.cpp\n",
    "# Since we don't need an LLM just yet, we'll be setting it to None\n",
    "\n",
    "# from llama_index import ServiceContext\n",
    "# Uncomment the line above and comment away the line below if you face an import error\n",
    "from llama_index.core import ServiceContext\n",
    "\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Connect Llama-Index and MongoDB Atlas\n",
    "\n",
    "As mentioned in the Quest, we'll be using MongoDB Atlas as our vector storage. This is critical to store indexed data and then query later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.mongodb import MongoDBAtlasVectorSearch\n",
    "# from llama_index.storage.storage_context import StorageContext\n",
    "# Uncomment the line above and comment away the line below if you face an import error\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "vector_store = MongoDBAtlasVectorSearch(mongodb_client = mongodb_client,\n",
    "                                 db_name = DB_NAME, collection_name = COLLECTION_NAME,\n",
    "                                 index_name  = INDEX_NAME)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Read PDF Documents\n",
    "\n",
    "Llama-index has very handy `SimpleDirectoryReader` that can read single/multiple files and also an entire directory's content. We'll be using this to read our 2 PDF files and storing the data in `docs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 128 chunks from '/home/engineerlambda/stackup-bounty/ELE562'\n",
      "CPU times: user 21 s, sys: 242 ms, total: 21.2 s\n",
      "Wall time: 23.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# from llama_index.readers.file.base import SimpleDirectoryReader\n",
    "# Uncomment the line above and comment away the line below if you face an import error\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "cwd = os.getcwd()\n",
    "data_dir = os.path.join(cwd, 'ELE562')\n",
    "\n",
    "docs = SimpleDirectoryReader(\n",
    "        input_dir=data_dir\n",
    ").load_data()\n",
    "\n",
    "print (f\"Loaded {len(docs)} chunks from '{data_dir}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Index the Documents and Store Them Into MongoDB Atlas\n",
    "\n",
    "The code cell below is where everything that we've been preparing for in this Python notebook comes together:\n",
    "- Embeddings are generated using our packaged-up embedding model `service_context` \n",
    "- Our documents `docs` get indexed `storage_context` - both text and embeddings are stored in MongoDB Atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.06 s, sys: 128 ms, total: 3.19 s\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# from llama_index.indices.vector_store.base import VectorStoreIndex\n",
    "# Uncomment the line above and comment away the line below if you face an import error\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    docs, \n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bounty",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
